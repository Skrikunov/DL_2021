#!/usr/bin/env python
# coding: utf-8


import torch
from torch.autograd import Variable
import numpy as np 
import unittest


def make_tester(test_classes):
    # test_classes: dict (layer name -> class implementing with layer)

    # Making a tester class which depends on test_classes dict:
    class _tester(unittest.TestCase):
        def test_Linear(self):
            np.random.seed(42)
            torch.manual_seed(42)

            batch_size, n_in, n_out = 2, 3, 4
            for _ in range(100):
                # layers initialization
                torch_layer = torch.nn.Linear(n_in, n_out)
                custom_layer = test_classes['Linear'](n_in, n_out)
                custom_layer.W = torch_layer.weight.data.numpy()
                custom_layer.b = torch_layer.bias.data.numpy()

                layer_input = np.random.uniform(-10, 10, (batch_size, n_in)).astype(np.float32)
                next_layer_grad = np.random.uniform(-10, 10, (batch_size, n_out)).astype(np.float32)

                # 1. check layer output
                custom_layer_output = custom_layer.updateOutput(layer_input)
                layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)
                torch_layer_output_var = torch_layer(layer_input_var)
                self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-3))
            
                # 2. check layer input grad
                custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)
                torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))
                torch_layer_grad_var = layer_input_var.grad
                self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-3))

                # 3. check layer parameters grad
                custom_layer.accGradParameters(layer_input, next_layer_grad)
                weight_grad = custom_layer.gradW
                bias_grad = custom_layer.gradb
                torch_weight_grad = torch_layer.weight.grad.data.numpy()
                torch_bias_grad = torch_layer.bias.grad.data.numpy()
                self.assertTrue(np.allclose(torch_weight_grad, weight_grad, atol=1e-6))
                self.assertTrue(np.allclose(torch_bias_grad, bias_grad, atol=1e-6))

        def test_SoftMax(self):
            np.random.seed(42)
            torch.manual_seed(42)

            batch_size, n_in = 2, 4
            for _ in range(100):
                # layers initialization
                torch_layer = torch.nn.Softmax(dim=-1)
                custom_layer = test_classes['SoftMax']()

                layer_input = np.random.uniform(-10, 10, (batch_size, n_in)).astype(np.float32)
                next_layer_grad = np.random.random((batch_size, n_in)).astype(np.float32)
                next_layer_grad /= next_layer_grad.sum(axis=-1, keepdims=True)
                next_layer_grad = next_layer_grad.clip(1e-5,1.)
                next_layer_grad = 1. / next_layer_grad

                # 1. check layer output
                custom_layer_output = custom_layer.updateOutput(layer_input)
                layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)
                torch_layer_output_var = torch_layer(layer_input_var)
                self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-3))

                # 2. check layer input grad
                custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)
                torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))
                torch_layer_grad_var = layer_input_var.grad
                self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-3))
                
        def test_LogSoftMax(self):
            np.random.seed(42)
            torch.manual_seed(42)

            batch_size, n_in = 2, 4
            for _ in range(100):
                # layers initialization
                torch_layer = torch.nn.LogSoftmax(dim=-1)
                custom_layer = test_classes['LogSoftMax']()

                layer_input = np.random.uniform(-10, 10, (batch_size, n_in)).astype(np.float32)
                next_layer_grad = np.random.random((batch_size, n_in)).astype(np.float32)
                next_layer_grad /= next_layer_grad.sum(axis=-1, keepdims=True)

                # 1. check layer output
                custom_layer_output = custom_layer.updateOutput(layer_input)
                layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)
                torch_layer_output_var = torch_layer(layer_input_var)
                self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-3))

                # 2. check layer input grad
                custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)
                torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))
                torch_layer_grad_var = layer_input_var.grad
                self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-3))

        def test_BatchNormalization(self):
            np.random.seed(42)
            torch.manual_seed(42)

            batch_size, n_in = 32, 16
            for _ in range(100):
                # layers initialization
                slope = np.random.uniform(0.01, 0.05)
                alpha = 0.9
                custom_layer = test_classes['BatchNormalization'](alpha)
                custom_layer.train()
                torch_layer = torch.nn.BatchNorm1d(n_in, eps=custom_layer.EPS, momentum=1.-alpha, affine=False)
                custom_layer.moving_mean = torch_layer.running_mean.numpy().copy()
                custom_layer.moving_variance = torch_layer.running_var.numpy().copy()

                layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)
                next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)

                # 1. check layer output
                custom_layer_output = custom_layer.updateOutput(layer_input)
                layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)
                torch_layer_output_var = torch_layer(layer_input_var)
                self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-3))

                # 2. check layer input grad
                custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)
                torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))
                torch_layer_grad_var = layer_input_var.grad
                self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-3))

                # 3. check moving mean
                self.assertTrue(np.allclose(custom_layer.moving_mean, torch_layer.running_mean.numpy()))
                # we don't check moving_variance because pytorch uses slightly different formula for it:
                # it computes moving average for unbiased variance (i.e var*N/(N-1))
                #self.assertTrue(np.allclose(custom_layer.moving_variance, torch_layer.running_var.numpy()))

                # 4. check evaluation mode
                custom_layer.moving_variance = torch_layer.running_var.numpy().copy()
                custom_layer.evaluate()
                custom_layer_output = custom_layer.updateOutput(layer_input)
                torch_layer.eval()
                torch_layer_output_var = torch_layer(layer_input_var)
                self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-3))
                
        def test_Sequential_BatchNorm_ChannelwiseScaling(self):
            np.random.seed(42)
            torch.manual_seed(42)

            batch_size, n_in = 2, 4
            for _ in range(100):
                # layers iniialization
                alpha = 0.9
                torch_layer = torch.nn.BatchNorm1d(n_in, eps=test_classes['BatchNormalization'].EPS,
                                                   momentum=1.-alpha, affine=True)
                torch_layer.bias.data = torch.from_numpy(np.random.random(n_in).astype(np.float32))
                custom_layer = test_classes['Sequential']()
                bn_layer = test_classes['BatchNormalization'](alpha)
                bn_layer.moving_mean = torch_layer.running_mean.numpy().copy()
                bn_layer.moving_variance = torch_layer.running_var.numpy().copy()
                custom_layer.add(bn_layer)
                scaling_layer = test_classes['ChannelwiseScaling'](n_in)
                scaling_layer.gamma = torch_layer.weight.data.numpy()
                scaling_layer.beta = torch_layer.bias.data.numpy()
                custom_layer.add(scaling_layer)
                custom_layer.train()

                layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)
                next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)

                # 1. check layer output
                custom_layer_output = custom_layer.updateOutput(layer_input)
                layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)
                torch_layer_output_var = torch_layer(layer_input_var)
                self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-3))

                # 2. check layer input grad
                custom_layer_grad = custom_layer.backward(layer_input, next_layer_grad)
                torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))
                torch_layer_grad_var = layer_input_var.grad
                self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-3))

                # 3. check layer parameters grad
                weight_grad, bias_grad = custom_layer.getGradParameters()[1]
                torch_weight_grad = torch_layer.weight.grad.data.numpy()
                torch_bias_grad = torch_layer.bias.grad.data.numpy()
                self.assertTrue(np.allclose(torch_weight_grad, weight_grad, atol=1e-3))
                self.assertTrue(np.allclose(torch_bias_grad, bias_grad, atol=1e-3))
        
        def test_Sequential_Linear_LeakyReLU(self):
            np.random.seed(42)
            torch.manual_seed(42)

            batch_size, n_in = 2, 3
            for _ in range(100):
                # layers initialization
                alpha = 0.01
                torch_layer = torch.nn.Sequential(
                    torch.nn.Linear(n_in, n_in),
                    torch.nn.LeakyReLU(negative_slope=alpha)
                )
                    
                torch_layer[0].bias.data = torch.from_numpy(
                    (np.random.rand(n_in).astype(np.float32) - 0.5) * 5
                )
                torch_layer[0].weight.data = torch.from_numpy(
                    (np.random.rand(n_in, n_in).astype(np.float32) - 0.5) * 5
                )

                layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)
                next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)

               
                custom_layer = test_classes['Sequential']()
                
                layer = test_classes['Linear'](n_in, n_in)
                layer.W = torch_layer[0].weight.data.numpy()
                layer.b = torch_layer[0].bias.data.numpy()
                custom_layer.add(layer)
                
                activ = test_classes['LeakyReLU'](slope=alpha)
                custom_layer.add(activ)
                custom_layer.train()

                layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)
                next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)

                # 1. check layer output
                custom_layer_output = custom_layer.updateOutput(layer_input)
                layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)
                torch_layer_output_var = torch_layer(layer_input_var)
                self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-3))

                # 2. check layer input grad
                custom_layer_grad = custom_layer.backward(layer_input, next_layer_grad)
                torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))
                torch_layer_grad_var = layer_input_var.grad
                self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-3))

                # 3. check layer parameters grad
                weight_grad, bias_grad = custom_layer.getGradParameters()[0]
                torch_weight_grad = torch_layer[0].weight.grad.data.numpy()
                torch_bias_grad = torch_layer[0].bias.grad.data.numpy()
                self.assertTrue(np.allclose(torch_weight_grad, weight_grad, atol=1e-3))
                self.assertTrue(np.allclose(torch_bias_grad, bias_grad, atol=1e-3))
        
#         def test_Sequential_my(self):
#             np.random.seed(42)
#             torch.manual_seed(42)

#             batch_size, n_in = 2, 4
#             for _ in range(100):
#                 # layers initialization
#                 alpha = 0.9
#                 torch_layer = torch.nn.Linear(n_in, n_in)
                
#                 custom_layer = test_classes['Sequential']()
                
#                 layer = test_classes['Linear'](n_in, n_in)
#                 layer.W = torch_layer.weight.data.numpy()
#                 layer.b = torch_layer.bias.data.numpy()
#                 custom_layer.add(layer)
                
#                 custom_layer.train()

#                 layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)
#                 next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)

#                 # 1. check layer output
#                 custom_layer_output = custom_layer.updateOutput(layer_input)
#                 layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)
#                 torch_layer_output_var = torch_layer(layer_input_var)
#                 self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-3))

#                 # 2. check layer input grad
#                 custom_layer_grad = custom_layer.backward(layer_input, next_layer_grad)
#                 torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))
#                 torch_layer_grad_var = layer_input_var.grad
#                 self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-3))

#                 # 3. check layer parameters grad
#                 weight_grad, bias_grad = custom_layer.getGradParameters()[0]
#                 torch_weight_grad = torch_layer.weight.grad.data.numpy()
#                 torch_bias_grad = torch_layer.bias.grad.data.numpy()
#                 self.assertTrue(np.allclose(torch_weight_grad, weight_grad, atol=1e-3))
#                 self.assertTrue(np.allclose(torch_bias_grad, bias_grad, atol=1e-3))
#                 """
#                 custom_layer.accGradParameters(layer_input, next_layer_grad)
#                 weight_grad = custom_layer.gradW
#                 bias_grad = custom_layer.gradb
#                 torch_weight_grad = torch_layer.weight.grad.data.numpy()
#                 torch_bias_grad = torch_layer.bias.grad.data.numpy()
#                 self.assertTrue(np.allclose(torch_weight_grad, weight_grad, atol=1e-6))
#                 self.assertTrue(np.allclose(torch_bias_grad, bias_grad, atol=1e-6))
#                 """
        
        def test_Dropout(self):
            np.random.seed(42)

            batch_size, n_in = 2, 4
            for _ in range(100):
                # layers initialization
                p = np.random.uniform(0.3, 0.7)
                layer = test_classes['Dropout'](p)
                layer.train()

                layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)
                next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)

                # 1. check layer output
                layer_output = layer.updateOutput(layer_input)
                self.assertTrue(np.all(np.logical_or(np.isclose(layer_output, 0), 
                                            np.isclose(layer_output*(1.-p), layer_input))))

                # 2. check layer input grad
                layer_grad = layer.updateGradInput(layer_input, next_layer_grad)
                self.assertTrue(np.all(np.logical_or(np.isclose(layer_grad, 0), 
                                            np.isclose(layer_grad*(1.-p), next_layer_grad))))

                # 3. check evaluation mode
                layer.evaluate()
                layer_output = layer.updateOutput(layer_input)
                self.assertTrue(np.allclose(layer_output, layer_input))

                # 4. check mask
                p = 0.0
                layer = test_classes['Dropout'](p)
                layer.train()
                layer_output = layer.updateOutput(layer_input)
                self.assertTrue(np.allclose(layer_output, layer_input))

                p = 0.5
                layer = test_classes['Dropout'](p)
                layer.train()
                layer_input = np.random.uniform(5, 10, (batch_size, n_in)).astype(np.float32)
                next_layer_grad = np.random.uniform(5, 10, (batch_size, n_in)).astype(np.float32)
                layer_output = layer.updateOutput(layer_input)
                zeroed_elem_mask = np.isclose(layer_output, 0)
                layer_grad = layer.updateGradInput(layer_input, next_layer_grad)        
                self.assertTrue(np.all(zeroed_elem_mask == np.isclose(layer_grad, 0)))

                # 5. dropout mask should be generated independently for every input matrix element, not for row/column
                batch_size, n_in = 1000, 1
                p = 0.8
                layer = test_classes['Dropout'](p)
                layer.train()

                layer_input = np.random.uniform(5, 10, (batch_size, n_in)).astype(np.float32)
                layer_output = layer.updateOutput(layer_input)
                self.assertTrue(np.sum(np.isclose(layer_output, 0)) != layer_input.size)

                layer_input = layer_input.T
                layer_output = layer.updateOutput(layer_input)
                self.assertTrue(np.sum(np.isclose(layer_output, 0)) != layer_input.size)
        
        def test_LeakyReLU(self):
            np.random.seed(42)
            torch.manual_seed(42)

            batch_size, n_in = 2, 4
            for _ in range(100):
                # layers initialization
                slope = np.random.uniform(0.01, 0.05)
                torch_layer = torch.nn.LeakyReLU(slope)
                custom_layer = test_classes['LeakyReLU'](slope)

                layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)
                next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)

                # 1. check layer output
                custom_layer_output = custom_layer.updateOutput(layer_input)
                layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)
                torch_layer_output_var = torch_layer(layer_input_var)
                self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-3))

                # 2. check layer input grad
                custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)
                torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))
                torch_layer_grad_var = layer_input_var.grad
                self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-3))

        def test_ELU(self):
            np.random.seed(42)
            torch.manual_seed(42)

            batch_size, n_in = 2, 4
            for _ in range(100):
                # layers initialization
                alpha = 1.0
                torch_layer = torch.nn.ELU(alpha)
                custom_layer = test_classes['ELU'](alpha)

                layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)
                next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)

                # 1. check layer output
                custom_layer_output = custom_layer.updateOutput(layer_input)
                layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)
                torch_layer_output_var = torch_layer(layer_input_var)
                self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-3))

                # 2. check layer input grad
                custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)
                torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))
                torch_layer_grad_var = layer_input_var.grad
                self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-3))

        def test_SoftPlus(self):
            np.random.seed(42)
            torch.manual_seed(42)

            batch_size, n_in = 2, 4
            for _ in range(100):
                # layers initialization
                torch_layer = torch.nn.Softplus()
                custom_layer = test_classes['SoftPlus']()

                layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)
                next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)

                # 1. check layer output
                custom_layer_output = custom_layer.updateOutput(layer_input)
                layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)
                torch_layer_output_var = torch_layer(layer_input_var)
                self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-3))

                # 2. check layer input grad
                custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)
                torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))
                torch_layer_grad_var = layer_input_var.grad
                self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-3))

        def test_ClassNLLCriterionUnstable(self):
            np.random.seed(42)
            torch.manual_seed(42)

            batch_size, n_in = 2, 4
            for _ in range(100):
                # layers initialization
                torch_layer = torch.nn.NLLLoss()
                custom_layer = test_classes['ClassNLLCriterionUnstable']()

                layer_input = np.random.uniform(0, 1, (batch_size, n_in)).astype(np.float32)
                layer_input /= layer_input.sum(axis=-1, keepdims=True)
                layer_input = layer_input.clip(custom_layer.EPS, 1. - custom_layer.EPS)  # unifies input
                target_labels = np.random.choice(n_in, batch_size)
                target = np.zeros((batch_size, n_in), np.float32)
                target[np.arange(batch_size), target_labels] = 1  # one-hot encoding

                # 1. check layer output
                custom_layer_output = custom_layer.updateOutput(layer_input, target)
                layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)
                torch_layer_output_var = torch_layer(torch.log(layer_input_var), 
                                                    Variable(torch.from_numpy(target_labels), requires_grad=False))
                self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-3))

                # 2. check layer input grad
                custom_layer_grad = custom_layer.updateGradInput(layer_input, target)
                torch_layer_output_var.backward()
                torch_layer_grad_var = layer_input_var.grad
                self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-3))

        def test_ClassNLLCriterion(self):
            np.random.seed(42)
            torch.manual_seed(42)

            batch_size, n_in = 2, 4
            for _ in range(100):
                # layers initialization
                torch_layer = torch.nn.NLLLoss()
                custom_layer = test_classes['ClassNLLCriterion']()

                layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)
                layer_input = torch.nn.LogSoftmax(dim=-1)(Variable(torch.from_numpy(layer_input))).data.numpy()
                target_labels = np.random.choice(n_in, batch_size)
                target = np.zeros((batch_size, n_in), np.float32)
                target[np.arange(batch_size), target_labels] = 1  # one-hot encoding

                # 1. check layer output
                custom_layer_output = custom_layer.updateOutput(layer_input, target)
                layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)
                torch_layer_output_var = torch_layer(layer_input_var, 
                                                    Variable(torch.from_numpy(target_labels), requires_grad=False))
                self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-3))

                # 2. check layer input grad
                custom_layer_grad = custom_layer.updateGradInput(layer_input, target)
                torch_layer_output_var.backward()
                torch_layer_grad_var = layer_input_var.grad
                self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-3))

        def test_adam_optimizer(self):
            state = {}  
            config = {'learning_rate': 1e-3, 'beta1': 0.9, 'beta2':0.999, 'epsilon':1e-8}
            variables = [[np.arange(10).astype(np.float64)]]
            gradients = [[np.arange(10).astype(np.float64)]]
            test_classes['adam_optimizer'](variables, gradients, config, state)
            self.assertTrue(np.allclose(state['m'][0], np.array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 
                                                                0.6, 0.7, 0.8, 0.9])))
            self.assertTrue(np.allclose(state['v'][0], np.array([0., 0.001, 0.004, 0.009, 0.016, 0.025, 
                                                                0.036, 0.049, 0.064, 0.081])))
            self.assertTrue(state['t'] == 1)
            self.assertTrue(np.allclose(variables[0][0], np.array([0., 0.999, 1.999, 2.999, 3.999, 4.999, 
                                                                5.999, 6.999, 7.999, 8.999])))
            test_classes['adam_optimizer'](variables, gradients, config, state)
            self.assertTrue(np.allclose(state['m'][0], np.array([0., 0.19, 0.38, 0.57, 0.76, 0.95, 1.14, 
                                                                1.33, 1.52, 1.71])))
            self.assertTrue(np.allclose(state['v'][0], np.array([0., 0.001999, 0.007996, 0.017991, 
                                                                0.031984, 0.049975, 0.071964, 0.097951, 
                                                                0.127936, 0.161919])))
            self.assertTrue(state['t'] == 2)
            self.assertTrue(np.allclose(variables[0][0], np.array([0., 0.998, 1.998, 2.998, 3.998, 4.998, 
                                                                   5.998, 6.998, 7.998, 8.998])))

    return _tester


def make_advanced_tester(test_classes):
    # test_classes: dict (layer name -> class implementing with layer)

    # Making a tester class which depends on test_classes dict:
    class _tester(unittest.TestCase):
        def test_Conv2d(self):
            np.random.seed(42)
            torch.manual_seed(42)

            batch_size, n_in, n_out = 2, 3, 4
            h,w = 5,6
            kern_size = 3
            for _ in range(100):
                # layers initialization
                torch_layer = torch.nn.Conv2d(n_in, n_out, kern_size, padding=1)
                custom_layer = test_classes['Conv2d'](n_in, n_out, kern_size)
                custom_layer.W = torch_layer.weight.data.numpy() # [n_out, n_in, kern, kern]
                custom_layer.b = torch_layer.bias.data.numpy()

                layer_input = np.random.uniform(-1, 1, (batch_size, n_in, h,w)).astype(np.float32)
                next_layer_grad = np.random.uniform(-1, 1, (batch_size, n_out, h, w)).astype(np.float32)

                # 1. check layer output
                custom_layer_output = custom_layer.updateOutput(layer_input)
                layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)
                torch_layer_output_var = torch_layer(layer_input_var)
                self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))
            
                # 2. check layer input grad
                custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)
                torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))
                torch_layer_grad_var = layer_input_var.grad
                self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6))
                
                # 3. check layer parameters grad
                custom_layer.accGradParameters(layer_input, next_layer_grad)
                weight_grad = custom_layer.gradW
                bias_grad = custom_layer.gradb
                torch_weight_grad = torch_layer.weight.grad.data.numpy()
                torch_bias_grad = torch_layer.bias.grad.data.numpy()
                #m = ~np.isclose(torch_weight_grad, weight_grad, atol=1e-5)
                self.assertTrue(np.allclose(torch_weight_grad, weight_grad, atol=1e-6, ))
                self.assertTrue(np.allclose(torch_bias_grad, bias_grad, atol=1e-6))
                
        def test_MaxPool2d(self):
            np.random.seed(42)
            torch.manual_seed(42)

            batch_size, n_in = 2, 3
            h,w = 4,6
            kern_size = 2
            for _ in range(100):
                # layers initialization
                torch_layer = torch.nn.MaxPool2d(kern_size)
                custom_layer = test_classes['MaxPool2d'](kern_size)

                layer_input = np.random.uniform(-10, 10, (batch_size, n_in, h,w)).astype(np.float32)
                next_layer_grad = np.random.uniform(-10, 10, (batch_size, n_in, 
                                                            h // kern_size, w // kern_size)).astype(np.float32)

                # 1. check layer output
                custom_layer_output = custom_layer.updateOutput(layer_input)
                layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)
                torch_layer_output_var = torch_layer(layer_input_var)
                self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))
            
                # 2. check layer input grad
                custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)
                torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))
                torch_layer_grad_var = layer_input_var.grad
                self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6))
    
    return _tester
