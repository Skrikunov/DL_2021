{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Part 2.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"CFYpvF5NYJDE"},"source":["# Homework 4\n","## Part 2 *(12 points)*\n","\n","When solving some problem with deep learning in practice, you search the Web for the latest paper on that problem, and take its implementation from GitHub. However, often authors won't publish code, so being able to **reimplement a paper** is a vital skill. You will likely have to do it in your course project.\n","\n","In this assignment, we'll simulate reimplementing a paper (in a greatly simplified environment though). As in many papers, some details of the system may be omitted so you'll have to leverage intuition and guess.\n","\n","Remember that there's already some great intuition on the Web (similar papers, Reddit, GitHub...). For example, if you have difficulties optimizing Transformer, it's not just you: googling \"*transformer training diverges*\" will show discussions and even papers just on Transformer training tricks. Still, **cite any foreign code** that you use (although this time you'll likely won't need any)."]},{"cell_type":"markdown","metadata":{"id":"zJvx8lEoHGFB"},"source":["## Problem\n","\n","In this part, you'll create an out-of-the-box text-to-speech (**TTS**) system:\n","\n","![](https://user-images.githubusercontent.com/9570420/81783573-392ed600-9504-11ea-98da-86ac05457c29.png)"]},{"cell_type":"markdown","metadata":{"id":"pgtqnCK_ptU1"},"source":["## Task\n","\n","**Reimplement and train the TTS system defined in \"*Method*\" and write a report.**\n","\n","### Deliverables\n","\n","1. `part2_solution.py`\n","1. `TTS.pth` (uploaded to Drive as in the assignment â„–2)\n","1. `report.pdf`\n","\n","### Expected Results\n","\n","* \"Run All\" must work without errors with both `DO_TRAIN = True` and `DO_TRAIN = False`.\n","* The implementation **must agree with everything in \"*Method*\"**.\n","* There should be more of tone/intonation and less of \"metallic\"/\"tin sounding\"/\"robotic\" voice.\n","* [train](https://drive.google.com/file/d/1xjF47mSQ-1XkvxM4emubpCkDSJHKZHcr/view?usp=sharing), [validation](https://drive.google.com/file/d/18yTzSpYA1PKeswuyUowTPMXvX7azYnZA/view?usp=sharing) â€” examples of insufficient quality.\n","* [train](https://drive.google.com/file/d/1W72htJ1NVmS-mvaVDqINUIVzGWhD8ScR/view?usp=sharing), [validation](https://drive.google.com/file/d/1KDiP4KQCJJpsf-Q_NZnwY7jck2QHeihx/view?usp=sharing) â€” aim to have at least this quality at 13 000 training iterations. For full marks, your audios should be no worse than this.\n","* [train](https://drive.google.com/file/d/1ZyGEE8Whvs89ojSRiqkaMi82owiPHQmk/view?usp=sharing), [validation](https://drive.google.com/file/d/1dcdCxTKcCSy8I6EeU-aXJdyLcC3xLp2K/view?usp=sharing) â€” if you got this or better (note how smooth now are the words \"arrogance and hostility\"), you're awesome! Provided great patience, expect something like this at 35 000 iterations. This isn't obligatory, but still is a great achievement and guarantees \"passing the quality check\".\n","* Sorry, it's hard to come up with formal assessment criteria for audio samples. We'll rely on common sense.\n","\n","### Report\n","\n","* Explain some worst bugs that you had to fix.\n","* Tell your story of training runs. **Attach screenshots of loss curves.** How many runs in total have you done? What was the longest one? Did you have to use optimization tricks/heuristics/magic from the Web?\n","* Was it easy to obtain the required audio quality? Do you think the best model could improve if trained for longer?\n","* For you personally, what were the **top 3 hardest things** in this assignment? (name anything, even \"*waiting to be unblocked on Colab*\")\n","* What have you learned while doing this task?\n","* What questions of yours has this task provoked? E.g. \"I was curious if Transformers are used in X\" or \"I'm wondering if real papers really don't mention Y\".\n","* How would you improve this assignment?"]},{"cell_type":"markdown","metadata":{"id":"NDkpVP7L17Cs"},"source":["## Data\n","\n"," Intuitively, in the real world your dataset would be a set of pairs:\n","\n","* Input:\n","  * `text` *(string)*.\n","* Target:\n","  * raw audio of a person saying `text` *(array of amplitude values sampled e.g. 44100 times per second â€” see an example plotted above)*.\n","\n","However, you'll deal with a simplified dataset. One dataset sample is an utterance described by\n","\n","* Input:\n","  * list of [ARPAbet phonemes](http://www.speech.cs.cmu.edu/cgi-bin/cmudict#phones);\n","  * *phoneme alignment*, i.e. start time and duration for each phoneme.\n","* Target:\n","  * [mel spectrogram](https://pytorch.org/audio/stable/transforms.html#melspectrogram) (frequency representation of audio) of a person saying text.\n","\n","![image](https://user-images.githubusercontent.com/9570420/81795777-2a9cea80-9515-11ea-99eb-05915f803af1.png)"]},{"cell_type":"code","metadata":{"id":"tom5_oaswBXK"},"source":["# Download extra stuff\n","\n","from pathlib import Path\n","\n","# Dataset (2.2 GB download, needs ~5 GB of disk space)\n","if not Path(\"./LJ-Speech-aligned/\").is_dir():\n","    !gdown --id \"13ObzCKsQHPqTc_aJk_m4NsMW3ljs0okI\"\n","    if not Path(\"./LJ-Speech-aligned.zip\").is_file():\n","        print(\"\\n\\nCouldn't download from Google Drive, trying OneDrive\\n\\n\")\n","        !wget -O \"LJ-Speech-aligned.zip\" \"https://eduhseru-my.sharepoint.com/:u:/g/personal/eaburkov_edu_hse_ru/EbZTIu-yUdJJpWxuMmmOh88Bt8Q5tcv_wqUK7YJLs-EBxA?e=caQfgB&download=1\"\n","\n","    !unzip -q LJ-Speech-aligned.zip && rm LJ-Speech-aligned.zip\n","\n","# Code to work with that dataset\n","if not Path(\"./lj_speech.py\").is_file():\n","    !gdown --id 1k2wCeMkHqRUzkAsEnV1eq7EpzZ6wIyr1\n","\n","# Deep learning software that converts spectrograms to audio\n","if not Path(\"./waveglow/\").is_dir():\n","    !git clone --recursive https://github.com/shrubb/waveglow.git -b denoiser-fix\n","\n","if not Path(\"./waveglow_256channels_ljs_v2.pt\").is_file():\n","    !wget -c https://api.ngc.nvidia.com/v2/models/nvidia/waveglow_ljs_256channels/versions/2/files/waveglow_256channels_ljs_v2.pt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vlknjzQxv7ok"},"source":["Use this Python module to handle our dataset. It's documented, so when in doubt, use `help()` or read the code with `??lj_speech`.\n","\n","There are also some useful constants, check them with `?lj_speech`."]},{"cell_type":"code","metadata":{"id":"-Fl3btUqwBXM"},"source":["import lj_speech"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XLmKyaGEwBXN"},"source":["DATASET_ROOT = Path(\"./LJ-Speech-aligned/\")\n","train_dataset, val_dataset = lj_speech.get_dataset(DATASET_ROOT)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xHv9pTNZv7o4"},"source":["Here is an example datapoint:"]},{"cell_type":"code","metadata":{"id":"YxjI6_4PlUYz"},"source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","example_datapoint = train_dataset[666]\n","print(f\"Datasets yield: {', '.join(example_datapoint.keys())}\")\n","print(f\"Text: '{example_datapoint['text']}'\")\n","print(f\"Phonemes: '{' '.join(example_datapoint['phonemes_code'][:30])} ...'\")\n","plt.imshow(example_datapoint['spectrogram']);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EaNwF1j1v7o-"},"source":["To \"play\" spectrograms, including those that you will generate, we will use another deep learning algorithm called [WaveGlow](https://arxiv.org/abs/1811.00002). It converts mel spectrograms to audio.\n","\n","Fortunately, there *is* code for it on GitHub, so you won't have to reimplement it ðŸ™‚ Still, when you have free time, I encourage you to watch a short [video](https://www.youtube.com/watch?v=CqFIVCD1WWo) about WaveNet â€” the famous paper that WaveGlow is based on."]},{"cell_type":"code","metadata":{"id":"aCVdjPzDv7pG"},"source":["vocoder = lj_speech.Vocoder()\n","\n","print(example_datapoint['text'])\n","\n","example_spectrogram = example_datapoint['spectrogram']\n","audio = vocoder(example_spectrogram)\n","lj_speech.play_audio(audio)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O44_2AnZaQbr"},"source":["del vocoder # free GPU memory"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y5cG-sPVKwie"},"source":["Finally, we have phonemes as inputs, but we'd like to synthesize arbitrary text. For that, there is `lj_speech.text_to_phonemes(text)`."]},{"cell_type":"code","metadata":{"id":"jy6YReGYoaHo"},"source":["lj_speech.text_to_phonemes(\"I love you\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AV22STdav7pK"},"source":["## Method\n","\n","![pipeline](https://user-images.githubusercontent.com/9570420/116498026-73c68580-a8b1-11eb-927d-2605755814aa.png)\n","\n","### Architecture\n","\n","**Encoder** converts raw phonemes to their representations that include context. It's inspired by [this paper](https://arxiv.org/abs/1910.10352) and is a stack of Transformer encoder layers and convolutions. In contrast to that paper, we don't use layer normalization; instead, we replace 1D convolution with full residual blocks where batch normalization is done. We use additive learned positional encoding.\n","\n","**Duration model** takes encoder's rich phoneme representations and predicts the duration of each phoneme in spectrogram frames. It's a a recurrent network that's trained independently and only after the rest of the system has been trained.\n","\n","**Alignment model** \"upsamples\" phoneme embeddings according to their durations to match the output spectrogram shape. It employs Gaussian upsampling introduced in [\"Non-Attentive Tacotron\"](https://arxiv.org/abs/2010.04301) (section 3.1). The difference is that we learn the single $\\sigma$ for all inputs, and we compute PDFs at a more natural $t+0.5$ rather than at $t$.\n","\n","**Decoder** starts identical to encoder â€” with multi-head attention and convolutional blocks interleaved â€” except the smaller dropout rate. We even inject positional encodings again because the sequence length is now different and has a new meaning. After that, we predict the final spectrogram, imposing supervision at this point already. However, we found out that learnable post-processing greatly benefits quality, so we run the *post-processing network* that predicts a correction (residual) to be added to the above spectrogram. As a result, the output is supervised with a combination of two losses.\n","\n","### Implementation Details\n","\n","* It's important to not compute loss on the padded parts (both for spectrograms and durations) and to average it properly afterwards.\n","* Outputs and targets, especially durations, can take values uncomfortable for the neural network which can lead to poor quality of divergence. Therefore, they need to be carefully normalized.\n","* Recurrent nets and transformers tend to be unstable in training. It's important to [clip gradients](https://pytorch.org/docs/stable/nn.html#torch.nn.utils.clip_grad_norm_), use a suitable learning rate schedule (like \"Noam warmup\" for Transformers) and possibly employ other common tricks.\n","* For speed, it's critical that Gaussian sampling implementation is fully vectorized, i.e. **all** parts of it run on GPU and don't use Python loops.\n","* Because for one phoneme sequence there can be many correct spectrograms, pointwise spectrogram loss isn't fully representative of actual audio quality. We have noticed that even if the validation loss increases, the quality may still improve."]},{"cell_type":"markdown","metadata":{"id":"ffjfvqfHzISd"},"source":["## Tips\n","\n","* Track failures early. Stop bad experiments early. Test one new thing at once.\n","* Don't use code from part 1, use built-in PyTorch Transformer blocks.\n","* Large batches aren't always good. Especially if you have a tight deadline and Colab time limits.\n","* Send audio to TensorBoard (or whatever is your favourite logging tool).\n","* If you're on a GPU that natively supports mixed precision training (e.g. T4, P100, V100, RTX, GTX 16), you're lucky. Mixed precision training is built into PyTorch and speeds training up almost for free.\n","* Batches of too uneven lengths waste computations on padded parts. The technique known as \"bucketing\" can help."]},{"cell_type":"code","metadata":{"id":"zetnqJQ77AKr"},"source":["# Your solution\n","%load_ext autoreload\n","%autoreload 1\n","\n","%aimport part2_solution"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"waHvcoAVJsJQ"},"source":["# If `True`, will train the model from scratch.\n","# If `False`, instead of training will load weights from './TTS.pth'.\n","# When grading, we will test both cases.\n","DO_TRAIN = True"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U2FSODB-LyF3"},"source":["if DO_TRAIN:\n","    !rm -f TTS.pth\n","\n","    # Train from scratch for one epoch (to check that your training works)\n","    # and save the best model to \"./TTS.pth\"\n","    part2_solution.train_tts(DATASET_ROOT, num_epochs=1)\n","else:\n","    # Download the checkpoint and initialize model weights from it\n","    import urllib\n","    import subprocess\n","\n","    penalize = False\n","\n","    # Get your link and checksum\n","    claimed_md5_checksum, google_drive_link = part2_solution.get_checkpoint_metadata()\n","\n","    # Use your link to download \"TTS.pth\"\n","    !gdown --id {urllib.parse.urlparse(google_drive_link).path.split('/')[-2]}\n","\n","    try:\n","        # Compute the actual checksum\n","        real_md5_checksum = subprocess.check_output(\n","            [\"md5sum\", \"TTS.pth\"]).decode().split()[0]\n","    except subprocess.CalledProcessError as err:\n","        # Couldn't download or the filename isn't \"TTS.pth\"\n","        print(f\"Wrong link or filename: {err}\")\n","        penalize = True\n","    else:\n","        # The trained checkpoint is different from the one submitted\n","        if real_md5_checksum != claimed_md5_checksum:\n","            print(\"Checksums differ! Late submission?\")\n","            penalize = True\n","\n","    if penalize:\n","        raise ValueError(\"ðŸ”« Prepare the penalizer! ðŸ”«\")\n","\n","# Finally load weights\n","tts_synthesizer = part2_solution.TextToSpeechSynthesizer(\"./TTS.pth\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L7m2_gfvvbGJ"},"source":["phonemes = \"DH IH1 S pau pau pau IH1 Z pau pau pau S P AH1 R T AH1 AH1 AH1 pau\".split()\n","lj_speech.play_audio(\n","    tts_synthesizer.synthesize_from_phonemes(phonemes))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cscryy2PNg4w"},"source":["text = \"Pack my box with five dozen liquor jugs.\"\n","# text = \"The five boxing wizards jump quickly.\"\n","# text = \"The quick brown fox jumps over the lazy dog.\"\n","# text = \"How about some school tech.\"\n","# text = \"Last homework. We are in a deep trouble. No sleep tonight.\"\n","\n","lj_speech.play_audio(\n","    tts_synthesizer.synthesize_from_text(text))"],"execution_count":null,"outputs":[]}]}